---
title: "Practical Machine Learning week1"
author: "Dingchong"
date: "Friday, September 05, 2014"
output: html_document
---

#Week 1

##Prediction motivation (8:26)

Study design -train vs test   
Consceptual issues - out of sample error, ROC   
practical implementation - caret package   

##What is prediction? (8:39)


##Relative importance of steps (9:45)

###Components of a predictor:   
question -> input data -> features -> algorithm -> parameters -> evaluation   
###Features matter!
Properties of good features:   
Load to data compression   
Retain relevant information   
Are created based on expert application knowledge   
###Common mistakes
trying to automate feature selection    
not paying attention to data-specifc quirks:outliers   
### issues to consider
interpretable, simple, accurate, fast, scalable    
techblog.netflix.com/2014/04/netflix-recommendations-beyongd-5-stars.html

## In and out of sample errors (6:57)
issues about overfitting

## Prediction study design (9:05)

split data into: traning, testing, validation   
pick features: use cross-validation; pick prediction function: use cross-validation   
www.kaggle.com/   
avoid small sample sizes:    
for binary outcome, probability of perfect classification is approximately (1/2)^n.   
样本越大，碰巧正确的概率越小，而模型的作用越明显。   

拇指法则   
大样本：60%训练，20%测试，20%交叉检验；中样本：60-40%；小样本：cross validation   



##Types of errors (10:35)
### for descrete data
Sensitivity :TP(TP+FN)
specificity: TN/(FP+TN)
### for continuous data
MSE,RMSE

##Receiver Operating Characteristic (5:03)
ROC curve

##Cross validation (8:20)
1.use the origin training set,   
2.split it into training/test sets;   
3.build a model on the training set(new);   
4.evaluate on the test set;   
5.repeat and average the estimated errors  

Used for: pick varibales into model, type of function, parameters , comparing different predictors

types of corss validation: randomsampling, k-fold, leave one out

k-fold: larger k , less bias, more variance, smaller k, more bias, less variance   
random sampling must be done without replacement, or else is bootstrap(underestimates of the error)

##What data should you use? (6:01)


#Week 2

## Caret package (6:16)


## Data slicing (5:40)


## Training options (7:15)


## Plotting predictors (10:39)


## Basic preprocessing (10:52)


## Covariate creation (17:31)


## Preprocessing with principal components analysis (14:07)


## Predicting with Regression (12:22)


## Predicting with Regression Multiple Covariates (11:12)




```{r}
summary(cars)
```


