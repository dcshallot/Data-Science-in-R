---
title: "Practical Machine Learning week3"
author: "Dingchong"
date: "Wednesday, September 17, 2014"
output: html_document
---

# Week 3

## Predicting with trees (12:51)
## Bagging (9:13)
## Random Forests (6:49)
## Boosting (7:08)
## Model Based Prediction (11:39)


## Predicting with trees (12:51)

```{r}
data(iris)
table(iris$Species)
inTrain <- createDataPartition( y = iris$Species, p=0.75, list=F)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]

qplot( Petal.Width, Sepal.Width, color = Species, data= training)
# challenging for linear model

modFit <- train( Species ~ . , method="rpart", data= training)
print( modFit$finalModel)
plot( modFit$finalModel, uniform=T, main="classifcation tree")
text( modFit$finalModel, use.n=T, all= T, cex= 0.8)
library(rattle);library(rpart.plot)
fancyRpartPlot( modFit$finalModel)

# predict
predict( modFit, newdata= testing )
```

### Notes
1. they use interactions
2. data transformations less important
3. multiple tree building packages: party, rpart, tree


## Bagging (9:13)

### Bootstrap aggregating( bagging)
1.Resample cases and recalculate predictions   
2.Average or majority vote   
3.What about bagging of different models?
Notes: more userful for non-linear functions   
```{r}
library(ElemStatLearn); data( ozone, package="ElemStatLearn")
ozone <- ozone[ order(ozone$ozone), ]
head(ozone)
```
Bagged loess   
predict temp by ozone
```{r}
ll <- matrix(NA, nrow=10, ncol= 155 )
for ( i in 1:10 ) { # 10 times resamples with replacement
  ss <- sample( 1: dim(ozone)[1], replace = T ) 
  ozone0 <- ozone[ss, ]; ozone0 <- ozone0[ order(ozone0$ozone), ] 
 # smooth/regression using loess method
  loess0 <- loess( temperature ~ ozone, data = ozone0, span = 0.2 )
 # predict new data : ozone from 1: 155
  ll[i, ] <- predict( loess0, newdata=data.frame(ozone=1:155))
}
# 10 times predict for each ozone value

plot( ozone$ozone, ozone$temperature, pch=19, cex=0.5 )
for ( i in 1:10 ) { lines(1:155, ll[i,], col="grey", lwd=2)}
lines(1:155, apply(ll, 2, mean), col = "red", lwd =2)
# red line is the bag of all the grey lines

```



## Random Forests (6:49)

```{r}
rm(list=ls())
data(iris);library(ggplot2)
inTrain <- createDataPartition( y= iris$Species, p=0.7, list=F )
training <- iris[ inTrain, ]
testing <- iris[ -inTrain, ]

library(caret)
modFit <- train(Species~., data= training, method ="rf", prox=T )
modFit
getTree( modFit$finalModel, k= 2 )

```

## Boosting (7:08)

### Basic idea
1. take lots of weak predictors   
2. weight them and add them up   
3. get a stronger predictor   
adaboost   

### boosting in R
1. can be used with any subset of classifiers   
2. one large subclass is gradient boosting   
3. multiple libraries:   
  gbm: trees;   
  mboost: model based boosting;   
  ada: additive logistic regression;    
  gamBoost: boosting generalized additive models

```{r}
rm(list=ls())
library(ISLR); data(Wage); library(ggplot2);library(caret)
Wage <- subset(Wage, select = -c(logWage))
inTrain <- createDataPartition( y=Wage$wage, p = 0.7, list = F)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]

modFit <- train(wage ~., method="gbm", data = training, verbose=F)
print(modFit)

```

### Notes
A couple of nice tutorials for boosting   
- www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf   
- webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf   
Boosting, random forests and model ensembling are the most common tools to win Kaggle and other prediction contests( netflixprize ).   

## Model Based Prediction (11:39)




### methods in caret, see
http://topepo.github.io/caret/bytag.html

